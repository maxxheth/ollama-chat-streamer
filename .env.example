# Ollama Chat Streamer - Environment Configuration
# Copy this file to .env and customize the values

# =============================================================================
# OLLAMA CONNECTION
# =============================================================================

# The Ollama host URL
# Use http://localhost:11434 when running with network_mode: host (Linux)
# Use http://host.docker.internal:11434 for Mac/Windows without host networking
OLLAMA_HOST=http://localhost:11434

# The Ollama model to use (e.g., llama3, mistral, codellama, etc.)
OLLAMA_MODEL=llama3

# Comma-separated fallback models to try if the primary fails
OLLAMA_MODEL_FALLBACKS=

# =============================================================================
# CHAT LOGGING
# =============================================================================

# Path to the chat log file (relative or absolute)
CHAT_LOG_DEST=chat_log.txt

# =============================================================================
# FEATURE FLAGS
# =============================================================================

# Enable experimental features (true/false)
EXPERIMENTAL=false

# Enable intelligent web search - LLM decides when to search (true/false)
# Requires ddgs package (pip install ddgs)
EXPERIMENTAL_WEBSEARCH=false

# =============================================================================
# CONTEXT LOADING
# =============================================================================

# Path to a file or directory to load as context for the LLM
# If a directory, all files matching CONTEXT_GREP extensions will be loaded
CONTEXT_PATH=

# Comma-separated list of file extensions to include when loading from a directory
# Use "*" to include all files
CONTEXT_GREP=txt,log

# =============================================================================
# DATABASE PERSISTENCE (PostgreSQL)
# =============================================================================

# Enable saving conversations to PostgreSQL database (true/false)
# Requires: pip install psycopg2-binary asyncpg
PERSIST_TO_DB=false

# PostgreSQL connection URL
# Format: postgresql://username:password@host:port/database
# 
# For Docker Compose (recommended):
# DATABASE_URL=postgresql://postgres:postgres@localhost:5432/chatdb
#
# For local PostgreSQL:
# DATABASE_URL=postgresql://postgres:password@localhost:5432/chatdb
#
# For Docker Compose service:
# DATABASE_URL=postgresql://postgres:postgres@postgres:5432/chatdb
DATABASE_URL=

# =============================================================================
# RETRY / BACKOFF
# =============================================================================

# Max retry attempts for Ollama calls
RETRY_MAX_ATTEMPTS=3

# Initial backoff delay in seconds
RETRY_INITIAL_DELAY=0.5

# Max backoff delay in seconds
RETRY_MAX_DELAY=8.0

# Backoff multiplier
RETRY_MULTIPLIER=2.0

# Jitter ratio applied to backoff delay (0.1 = 10%)
RETRY_JITTER=0.1

# =============================================================================
# EXAMPLES
# =============================================================================

# Example 1: Basic setup with local Ollama
# OLLAMA_HOST=http://localhost:11434
# OLLAMA_MODEL=llama3
# OLLAMA_MODEL_FALLBACKS=mistral,codellama
# CHAT_LOG_DEST=./logs/chat.log

# Example 2: Enable web search for current information
# EXPERIMENTAL_WEBSEARCH=true

# Example 3: Load project documentation as context
# CONTEXT_PATH=./docs
# CONTEXT_GREP=md,txt

# Example 4: Load a specific file as context
# CONTEXT_PATH=./project_requirements.txt

# Example 5: Enable database persistence
# PERSIST_TO_DB=true
# DATABASE_URL=postgresql://postgres:postgres@localhost:5432/chatdb

# Example 6: Load past conversations from database
# CONTEXT_PATH=db

# Example 7: Load from database + additional file paths
# CONTEXT_PATH=db,./notes,./docs
